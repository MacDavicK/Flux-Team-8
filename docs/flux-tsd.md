# Flux — Backend Engineering Specification
### Complete Technical Reference for Claude Code

> **Version:** 2.0 — Greenfield (incorporates Architecture Review fixes)
> **Team:** 4 Backend Engineers
> **Python:** 3.12 · Package Manager: `uv`
> **Stack:** FastAPI · LangGraph · Supabase (PostgreSQL) · Twilio · LiteLLM · LangSmith · Sentry
> **Deployment:** Docker Compose (local) → Railway/Render (production)
> **Document Purpose:** This specification is the single source of truth for building the complete Flux backend. Every design decision, data model, agent contract, API endpoint, environment variable, and infrastructure concern is documented here.

---

## Table of Contents

1. [Product Overview](#1-product-overview)
2. [Repository Structure](#2-repository-structure)
3. [Tech Stack — Canonical Choices](#3-tech-stack--canonical-choices)
4. [Environment Variables](#4-environment-variables)
5. [Database Schema](#5-database-schema)
6. [Agent Architecture](#6-agent-architecture)
7. [LangGraph Orchestration](#7-langgraph-orchestration)
8. [LLM Integration via LiteLLM + OpenRouter](#8-llm-integration-via-litellm--openrouter)
9. [Notification Engine](#9-notification-engine)
10. [Analytics](#10-analytics)
11. [API Design — OpenAPI / Swagger](#11-api-design--openapi--swagger)
12. [Onboarding Flow](#12-onboarding-flow)
13. [Privacy, Security & Compliance](#13-privacy-security--compliance)
14. [Observability — LangSmith, Sentry, Structured Logging](#14-observability--langsmith-sentry-structured-logging)
15. [Cost Controls](#15-cost-controls)
16. [Rate Limiting](#16-rate-limiting)
17. [Docker & Deployment](#17-docker--deployment)
18. [Testing Strategy](#18-testing-strategy)
19. [Phase 2 Roadmap](#19-phase-2-roadmap)
20. [Open Decisions & Engineering Defaults](#20-open-decisions--engineering-defaults)

---

## 1. Product Overview

Flux is a conversational AI productivity companion. Users express goals and tasks in natural language; a multi-agent LLM pipeline parses intent, builds behaviorally-grounded plans, and escalates notifications progressively if the user doesn't follow through.

### Two Core Primitives

**Goals** — aspirations requiring a multi-week plan broken into sustainable daily tasks. Structured as 6-week sprints (rooted in Lally et al. 2010 habit-formation research and BJ Fogg's Tiny Habits framework), designed to maintain a >70% user success rate. Each goal produces a sequential pipeline of 6-week micro-goal sprints that unlock upon completion of the previous one.

**Tasks** — discrete, time-bound or location-triggered reminders. May be standalone or generated by a goal plan. Every task has a full notification escalation chain: push → WhatsApp → voice call → auto-miss.

### Chat as the Primary Interface

All task and goal creation, modification, and rescheduling is done through conversation. There are no forms. The backend is responsible for parsing intent, negotiating plans, writing to the database, and managing the full notification lifecycle.

---

## 2. Repository Structure

```
flux-backend/
├── app/
│   ├── main.py                    # FastAPI app entry point, router registration
│   ├── config.py                  # Settings via pydantic-settings
│   ├── dependencies.py            # Shared FastAPI dependencies (auth, db, etc.)
│   │
│   ├── api/                       # All HTTP route handlers
│   │   ├── v1/
│   │   │   ├── chat.py            # POST /chat/message, GET /chat/history
│   │   │   ├── goals.py           # Goal CRUD endpoints
│   │   │   ├── tasks.py           # Task CRUD + action endpoints
│   │   │   ├── analytics.py       # Analytics endpoints
│   │   │   ├── patterns.py        # Pattern Observer UI endpoints (MVP)
│   │   │   ├── account.py         # Account management, GDPR endpoints
│   │   │   ├── demo.py            # Demo mode endpoints (MVP)
│   │   │   └── webhooks.py        # Twilio webhook handlers
│   │
│   ├── agents/                    # All LangGraph agents
│   │   ├── state.py               # AgentState TypedDict definition
│   │   ├── graph.py               # LangGraph graph assembly
│   │   ├── orchestrator.py        # Orchestrator agent node
│   │   ├── goal_planner.py        # Goal Planning agent node
│   │   ├── classifier.py          # Classifier agent node
│   │   ├── scheduler.py           # Scheduler agent node
│   │   ├── pattern_observer.py    # Pattern Observer agent node
│   │   ├── onboarding.py          # Onboarding subgraph node
│   │   └── prompts/               # All system prompts as .txt files
│   │       ├── orchestrator.txt
│   │       ├── goal_planner.txt
│   │       ├── classifier.txt
│   │       ├── scheduler.txt
│   │       └── pattern_observer.txt
│   │
│   ├── models/                    # Pydantic models
│   │   ├── agent_outputs.py       # Pydantic models for every agent JSON response
│   │   ├── api_schemas.py         # Request/response models for all API endpoints
│   │   └── db_schemas.py          # Internal DB-mapped models (not ORM; raw SQL)
│   │
│   ├── services/                  # Business logic layer
│   │   ├── supabase.py            # Supabase client initialization + raw SQL helpers
│   │   ├── llm.py                 # LiteLLM wrapper with retry, cost tracking
│   │   ├── twilio_service.py      # Twilio WhatsApp + Voice dispatch
│   │   ├── push_service.py        # Web Push (VAPID) dispatch
│   │   ├── rrule_expander.py      # iCal RRULE → individual task row expansion
│   │   ├── context_manager.py     # Conversation context window management
│   │   └── analytics_service.py  # Analytics query helpers
│   │
│   └── middleware/
│       ├── auth.py                # JWT validation middleware
│       ├── rate_limit.py          # slowapi rate limiting
│       └── logging.py            # structlog request correlation ID injection
│
├── notifier/                      # Notifier worker — separate process/container
│   ├── main.py                    # APScheduler entry point
│   ├── poll.py                    # Core notification polling logic
│   ├── dispatch.py                # Channel dispatch (push, whatsapp, call)
│   └── recovery.py                # Startup recovery for stuck pending dispatches
│
├── migrations/                    # SQL migration files (applied to Supabase)
│   ├── 001_initial_schema.sql
│   ├── 002_indexes.sql
│   ├── 003_materialized_views.sql
│   ├── 004_rls_policies.sql
│   └── 005_triggers.sql
│
├── tests/
│   ├── unit/
│   ├── integration/
│   └── conftest.py
│
├── docker/
│   ├── Dockerfile.api
│   └── Dockerfile.notifier
│
├── docker-compose.yml
├── docker-compose.prod.yml
├── pyproject.toml                 # uv project file
├── uv.lock
└── .env.example
```

---

## 3. Tech Stack — Canonical Choices

| Layer | Technology | Version | Rationale |
|---|---|---|---|
| Language | Python | 3.12 | Latest stable; LangGraph native support |
| Package Manager | uv | latest | Fast, lockfile-based, reproducible installs |
| Web Framework | FastAPI | ≥0.111 | Async-first, automatic OpenAPI/Swagger docs |
| Agent Framework | LangGraph | ≥0.2 | Stateful graphs, conditional routing, thread memory, `Send()` fan-out |
| LLM Gateway | LiteLLM + OpenRouter | ≥1.40 | All LLM calls routed via OpenRouter (single API key, unified billing); LiteLLM handles fallback and cost tracking |
| LLM Tracing | LangSmith | ≥0.1 | One env var enables full agent observability |
| Error Tracking | Sentry | ≥2.0 | FastAPI + worker exception capture |
| Database | Supabase (PostgreSQL 15) | — | Auth, RLS, Realtime, PostgREST; hosted Postgres |
| DB Driver | asyncpg | ≥0.29 | Async PostgreSQL driver |
| DB Migrations | Raw SQL files | — | Applied via Supabase dashboard or `psql` |
| Background Jobs | APScheduler | ≥3.10 | SQLAlchemy PostgreSQL job store for persistence across restarts |
| Notification — WhatsApp | Twilio WhatsApp Business API | — | Single vendor for all escalation channels |
| Notification — Voice | Twilio Programmable Voice (TTS) | — | DTMF response handling |
| Notification — Web Push | pywebpush | ≥2.0 | VAPID key management + push dispatch |
| Phone Verification | Twilio Verify | — | 6-digit OTP; gates WhatsApp/voice dispatch |
| Rate Limiting | slowapi | ≥0.1.9 | FastAPI-native; Redis-backed per-user limits |
| Timezone Handling | pendulum | ≥3.0 | DST-aware, drop-in replacement for pytz |
| RRULE Expansion | python-dateutil | ≥2.9 | RFC 5545 RRULE parsing and occurrence expansion |
| Structured Logging | structlog | ≥24.0 | JSON logs with request correlation IDs |
| Data Validation | pydantic | ≥2.0 | Agent output models, request/response schemas |
| Settings | pydantic-settings | ≥2.0 | `.env`-backed typed configuration |
| HTTP Client | httpx | ≥0.27 | Async HTTP for internal service calls |
| Twilio SDK | twilio | ≥9.0 | WhatsApp, Voice, Verify |
| Testing | pytest + pytest-asyncio | — | Async test support |
| Containerization | Docker | — | `Dockerfile.api` + `Dockerfile.notifier` |
| OpenAPI Docs | FastAPI built-in | — | Auto-generated at `/docs` (Swagger UI) and `/redoc` |

### LLM Model Assignments

All models are accessed via **OpenRouter**. The LiteLLM model string prefix is `openrouter/<provider>/<model>`. A single `OPENROUTER_API_KEY` covers all providers.

| Agent | Underlying Provider | Model | Model String (LiteLLM via OpenRouter) | Rationale |
|---|---|---|---|---|
| Orchestrator | OpenAI | GPT-4o | `openrouter/openai/gpt-4o` | Strong intent classification + multi-turn reasoning |
| Goal Planner | Anthropic | Claude Sonnet 4 | `openrouter/anthropic/claude-sonnet-4-20250514` | Long-context planning, nuanced multi-turn negotiation |
| Classifier | OpenAI | GPT-4o-mini | `openrouter/openai/gpt-4o-mini` | Fast, cheap, structured JSON output |
| Scheduler | OpenAI | GPT-4o-mini | `openrouter/openai/gpt-4o-mini` | Slot-finding logic with structured output |
| Pattern Observer | OpenAI | GPT-4o-mini | `openrouter/openai/gpt-4o-mini` | Pattern summarization over historical data |
| Summarizer (context mgmt) | OpenAI | GPT-4o-mini | `openrouter/openai/gpt-4o-mini` | Conversation summarization to manage context windows |
| Notifier | None | N/A | N/A | Pure deterministic state machine — no LLM |

> **Model Pinning Rule:** Always pin exact model version strings (as above). Never use aliases like `claude-sonnet-latest` — these silently update and can change behavior and billing without warning.

> **Fallback Strategy (via LiteLLM + OpenRouter):** If `openrouter/openai/gpt-4o` is unavailable, fall back to `openrouter/anthropic/claude-sonnet-4-20250514`. If `openrouter/anthropic/claude-sonnet-4-20250514` is unavailable for the Goal Planner, fall back to `openrouter/openai/gpt-4o`. Configure fallbacks in `litellm.fallbacks` (see §8).

---

## 4. Environment Variables

All configuration is managed via `pydantic-settings`. Create `.env` from `.env.example`. Never commit secrets.

```bash
# ──────────────────────────────────────────────
# App
# ──────────────────────────────────────────────
APP_ENV=development                          # development | production
SECRET_KEY=<random-64-char-hex>             # Used for signing internal tokens
LOG_LEVEL=INFO

# ──────────────────────────────────────────────
# Supabase / PostgreSQL
# ──────────────────────────────────────────────
SUPABASE_URL=https://<project>.supabase.co
SUPABASE_ANON_KEY=<anon-key>
SUPABASE_SERVICE_ROLE_KEY=<service-role-key>
# Direct connection (port 5432) — required for asyncpg + LangGraph checkpointer
# Do NOT use the PgBouncer pooler URL (port 6543) — transaction-mode pooling
# is incompatible with asyncpg prepared statements and LangGraph checkpoint writes.
DATABASE_URL=postgresql+asyncpg://postgres:<password>@db.<project>.supabase.co:5432/postgres

# ──────────────────────────────────────────────
# LLM Providers — All via OpenRouter
# ──────────────────────────────────────────────
# Single API key covers OpenAI, Anthropic, and all other OpenRouter-supported models.
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=sk-or-...
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
# Optional: identify your app in OpenRouter's dashboard (used for analytics/rate limiting)
OPENROUTER_APP_NAME=Flux
OPENROUTER_APP_URL=https://yourapp.com

# ──────────────────────────────────────────────
# LangSmith (Agent Tracing)
# ──────────────────────────────────────────────
LANGCHAIN_TRACING_V2=true
LANGCHAIN_ENDPOINT=https://api.smith.langchain.com
LANGCHAIN_API_KEY=ls__...
LANGCHAIN_PROJECT=flux-production             # or flux-development

# ──────────────────────────────────────────────
# Sentry (Error Tracking)
# ──────────────────────────────────────────────
SENTRY_DSN=https://...@sentry.io/...
SENTRY_TRACES_SAMPLE_RATE=0.2               # 20% performance tracing in prod
SENTRY_ENVIRONMENT=production               # or development

# ──────────────────────────────────────────────
# Twilio
# ──────────────────────────────────────────────
TWILIO_ACCOUNT_SID=AC...
TWILIO_AUTH_TOKEN=...
TWILIO_WHATSAPP_FROM=whatsapp:+14155238886  # Sandbox or approved sender
TWILIO_VOICE_FROM=+1...                     # Verified Twilio voice number
TWILIO_VERIFY_SERVICE_SID=VA...             # Twilio Verify service SID
TWILIO_WEBHOOK_BASE_URL=https://api.yourapp.com  # Used to build DTMF callback URLs

# ──────────────────────────────────────────────
# Web Push (VAPID)
# ──────────────────────────────────────────────
VAPID_PRIVATE_KEY=<base64-encoded-private-key>
VAPID_PUBLIC_KEY=<base64-encoded-public-key>
VAPID_CLAIMS_EMAIL=mailto:admin@yourapp.com

# ──────────────────────────────────────────────
# Redis (Rate Limiting + APScheduler at scale)
# ──────────────────────────────────────────────
REDIS_URL=redis://localhost:6379/0

# ──────────────────────────────────────────────
# Notification Defaults (all configurable per user)
# ──────────────────────────────────────────────
REMINDER_LEAD_MINUTES=10
ESCALATION_WINDOW_MINUTES=2
NOTIFICATION_POLL_INTERVAL_SECONDS=60

# ──────────────────────────────────────────────
# Cost Controls
# ──────────────────────────────────────────────
MONTHLY_TOKEN_SOFT_LIMIT=500000             # Per user; warn at this level
MONTHLY_TOKEN_HARD_LIMIT=1000000            # Per user; degrade to cheaper model
MAX_CONVERSATION_MESSAGES=20                # Older messages are summarized
MAX_CONVERSATION_TOKENS=8000               # Trigger summarization above this

# ──────────────────────────────────────────────
# Business Logic
# ──────────────────────────────────────────────
MAX_ACTIVE_GOALS=3
GOAL_SPRINT_WEEKS=6
PATTERN_MISS_THRESHOLD=3                    # Consecutive misses before flagging avoidance
PATTERN_MIN_DATAPOINTS=3                    # Min data points before a pattern is reported
```

### `app/config.py`

```python
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")

    # App
    app_env: str = "development"
    secret_key: str
    log_level: str = "INFO"

    # Supabase
    supabase_url: str
    supabase_anon_key: str
    supabase_service_role_key: str
    database_url: str  # asyncpg direct connection

    # LLMs — all via OpenRouter
    openrouter_api_key: str
    openrouter_base_url: str = "https://openrouter.ai/api/v1"
    openrouter_app_name: str = "Flux"
    openrouter_app_url: str = ""

    # LangSmith
    langchain_tracing_v2: bool = True
    langchain_api_key: str
    langchain_project: str = "flux-development"

    # Sentry
    sentry_dsn: str
    sentry_traces_sample_rate: float = 0.2
    sentry_environment: str = "development"

    # Twilio
    twilio_account_sid: str
    twilio_auth_token: str
    twilio_whatsapp_from: str
    twilio_voice_from: str
    twilio_verify_service_sid: str
    twilio_webhook_base_url: str

    # Web Push
    vapid_private_key: str
    vapid_public_key: str
    vapid_claims_email: str

    # Redis
    redis_url: str = "redis://localhost:6379/0"

    # Notification defaults
    reminder_lead_minutes: int = 10
    escalation_window_minutes: int = 2
    notification_poll_interval_seconds: int = 60

    # Cost controls
    monthly_token_soft_limit: int = 500_000
    monthly_token_hard_limit: int = 1_000_000
    max_conversation_messages: int = 20
    max_conversation_tokens: int = 8_000

    # Business logic
    max_active_goals: int = 3
    goal_sprint_weeks: int = 6
    pattern_miss_threshold: int = 3
    pattern_min_datapoints: int = 3

settings = Settings()
```

---

## 5. Database Schema

All tables live in Supabase (PostgreSQL 15). Row Level Security (RLS) is enabled on every user-data table. Migrations are raw SQL files applied in order.

### Migration 001 — Initial Schema

```sql
-- Enable UUID extension
CREATE EXTENSION IF NOT EXISTS "pgcrypto";

-- ─────────────────────────────────────────────────────────────────
-- users
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE users (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    email                   TEXT UNIQUE NOT NULL,
    created_at              TIMESTAMPTZ DEFAULT now(),
    onboarded               BOOLEAN DEFAULT false,
    timezone                TEXT NOT NULL DEFAULT 'UTC',        -- IANA timezone string e.g. 'Asia/Dubai'
    phone_verified          BOOLEAN DEFAULT false,
    whatsapp_opt_in_at      TIMESTAMPTZ,                       -- NULL = not opted in
    push_subscription       JSONB,                             -- Web Push subscription object
    profile                 JSONB,
    -- profile schema:
    -- {
    --   "name": "Alex",
    --   "sleep_window": { "start": "23:00", "end": "07:00" },
    --   "work_hours": { "start": "09:00", "end": "18:00", "days": ["Mon","Tue","Wed","Thu","Fri"] },
    --   "chronotype": "morning" | "evening" | "neutral",
    --   "existing_commitments": [
    --     { "title": "Gym", "days": ["Tuesday"], "time": "19:00", "duration_minutes": 60 }
    --   ],
    --   "locations": { "home": "labeled_home", "work": "labeled_work" }
    -- }
    notification_preferences JSONB DEFAULT '{
        "phone_number": null,
        "whatsapp_opted_in": false,
        "reminder_lead_minutes": 10,
        "escalation_window_minutes": 2
    }'::jsonb,
    monthly_token_usage     JSONB DEFAULT '{
        "openai": 0,
        "anthropic": 0,
        "total": 0,
        "reset_at": null
    }'::jsonb                                                  -- Token usage tracking for cost controls
);

-- ─────────────────────────────────────────────────────────────────
-- goals
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE goals (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id                 UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    title                   TEXT NOT NULL,
    description             TEXT,
    class_tags              TEXT[] DEFAULT '{}',               -- ["Health", "Fitness"] from Classifier
    status                  TEXT NOT NULL CHECK (status IN ('active', 'completed', 'abandoned', 'pipeline')),
    parent_goal_id          UUID REFERENCES goals(id),         -- for micro-goal chains
    pipeline_order          INT,                               -- sequence within a micro-goal chain
    created_at              TIMESTAMPTZ DEFAULT now(),
    activated_at            TIMESTAMPTZ,
    completed_at            TIMESTAMPTZ,
    target_weeks            INT DEFAULT 6,
    plan_json               JSONB                              -- full negotiated plan snapshot
);

-- ─────────────────────────────────────────────────────────────────
-- tasks
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE tasks (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id                 UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    goal_id                 UUID REFERENCES goals(id),         -- NULL for standalone tasks
    title                   TEXT NOT NULL,
    description             TEXT,
    status                  TEXT NOT NULL DEFAULT 'pending'
                                CHECK (status IN ('pending', 'done', 'missed', 'rescheduled', 'cancelled')),
    scheduled_at            TIMESTAMPTZ,                       -- Always stored in UTC
    duration_minutes        INT,
    trigger_type            TEXT CHECK (trigger_type IN ('time', 'location')),
    location_trigger        TEXT,                              -- e.g. "away_from_home" (MVP: simulated)
    recurrence_rule         TEXT,                              -- iCal RRULE string, RFC 5545
    shared_with_goal_ids    UUID[] DEFAULT '{}',               -- Task belongs to multiple goals
    -- Notification state tracking (atomic CAS update to prevent double-fire)
    reminder_sent_at        TIMESTAMPTZ,                       -- Push sent timestamp
    whatsapp_sent_at        TIMESTAMPTZ,                       -- WhatsApp sent timestamp
    call_sent_at            TIMESTAMPTZ,                       -- Voice call sent timestamp
    completed_at            TIMESTAMPTZ,
    created_at              TIMESTAMPTZ DEFAULT now()
);

-- ─────────────────────────────────────────────────────────────────
-- conversations  (MUST be created before messages — FK dependency)
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE conversations (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id                 UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    langgraph_thread_id     TEXT UNIQUE NOT NULL,
    context_type            TEXT CHECK (context_type IN ('onboarding', 'goal', 'task', 'reschedule', 'general')),
    created_at              TIMESTAMPTZ DEFAULT now(),
    last_message_at         TIMESTAMPTZ
);

-- ─────────────────────────────────────────────────────────────────
-- messages  (NEW — fixes Architecture Review gap §4 "Chat Messages Have No Storage Table")
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE messages (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id         UUID NOT NULL REFERENCES conversations(id) ON DELETE CASCADE,
    role                    TEXT NOT NULL CHECK (role IN ('user', 'assistant', 'system', 'summary')),
    content                 TEXT NOT NULL,
    agent_node              TEXT,                              -- which agent produced this message
    tokens_used             INT DEFAULT 0,
    provider                TEXT,                              -- 'openai' | 'anthropic'
    created_at              TIMESTAMPTZ DEFAULT now()
);

-- ─────────────────────────────────────────────────────────────────
-- patterns
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE patterns (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id                 UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,
    pattern_type            TEXT NOT NULL,                     -- 'time_avoidance' | 'completion_streak' | 'category_performance'
    description             TEXT,                              -- Human-readable summary
    data                    JSONB,                             -- Raw signal data
    confidence              FLOAT CHECK (confidence BETWEEN 0.0 AND 1.0),
    created_at              TIMESTAMPTZ DEFAULT now(),
    updated_at              TIMESTAMPTZ DEFAULT now()
);

-- ─────────────────────────────────────────────────────────────────
-- notification_log
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE notification_log (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id                 UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    channel                 TEXT NOT NULL CHECK (channel IN ('push', 'whatsapp', 'call')),
    external_id             TEXT,                              -- Twilio MessageSid or CallSid (idempotency key)
    sent_at                 TIMESTAMPTZ DEFAULT now(),
    response                TEXT CHECK (response IN ('done', 'reschedule', 'missed', 'no_response')),
    responded_at            TIMESTAMPTZ
);

-- ─────────────────────────────────────────────────────────────────
-- dispatch_log  (NEW — idempotency + recovery for Notifier)
-- ─────────────────────────────────────────────────────────────────
CREATE TABLE dispatch_log (
    id                      UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    task_id                 UUID NOT NULL REFERENCES tasks(id) ON DELETE CASCADE,
    channel                 TEXT NOT NULL CHECK (channel IN ('push', 'whatsapp', 'call', 'auto_miss')),
    status                  TEXT NOT NULL DEFAULT 'pending'
                                CHECK (status IN ('pending', 'dispatched', 'failed')),
    created_at              TIMESTAMPTZ DEFAULT now(),
    dispatched_at           TIMESTAMPTZ,
    error                   TEXT
);
```

### Migration 002 — Indexes

```sql
-- tasks: Primary query patterns
CREATE INDEX idx_tasks_user_scheduled     ON tasks (user_id, scheduled_at);
CREATE INDEX idx_tasks_user_status        ON tasks (user_id, status);
CREATE INDEX idx_tasks_goal_id            ON tasks (goal_id) WHERE goal_id IS NOT NULL;
CREATE INDEX idx_tasks_notifier_push      ON tasks (scheduled_at, reminder_sent_at, status)
    WHERE status = 'pending' AND reminder_sent_at IS NULL;
CREATE INDEX idx_tasks_notifier_whatsapp  ON tasks (reminder_sent_at, whatsapp_sent_at, status)
    WHERE status = 'pending' AND whatsapp_sent_at IS NULL;
CREATE INDEX idx_tasks_notifier_call      ON tasks (whatsapp_sent_at, call_sent_at, status)
    WHERE status = 'pending' AND call_sent_at IS NULL;

-- goals: User + status queries
CREATE INDEX idx_goals_user_status        ON goals (user_id, status);
CREATE INDEX idx_goals_parent             ON goals (parent_goal_id) WHERE parent_goal_id IS NOT NULL;

-- messages: Conversation history retrieval
CREATE INDEX idx_messages_conversation    ON messages (conversation_id, created_at);

-- patterns: User pattern lookup
CREATE INDEX idx_patterns_user_type       ON patterns (user_id, pattern_type);

-- notification_log: Idempotency lookup
CREATE UNIQUE INDEX idx_notification_log_external_id ON notification_log (external_id)
    WHERE external_id IS NOT NULL;

-- dispatch_log: Recovery queries
CREATE INDEX idx_dispatch_log_pending     ON dispatch_log (status, created_at)
    WHERE status = 'pending';
```

### Migration 003 — Materialized Views (Analytics)

```sql
-- Weekly completion rate per user
CREATE MATERIALIZED VIEW user_weekly_stats AS
SELECT
    user_id,
    DATE_TRUNC('week', scheduled_at) AS week,
    COUNT(*) FILTER (WHERE status = 'done')  AS completed,
    COUNT(*)                                  AS total,
    ROUND(
        COUNT(*) FILTER (WHERE status = 'done')::numeric / NULLIF(COUNT(*), 0) * 100,
    1) AS completion_pct
FROM tasks
WHERE scheduled_at IS NOT NULL
GROUP BY user_id, DATE_TRUNC('week', scheduled_at);

CREATE UNIQUE INDEX ON user_weekly_stats (user_id, week);

-- Missed tasks by goal category
CREATE MATERIALIZED VIEW missed_by_category AS
SELECT
    t.user_id,
    UNNEST(g.class_tags)                             AS category,
    COUNT(*) FILTER (WHERE t.status = 'missed')      AS missed_count,
    COUNT(*)                                          AS total_count
FROM tasks t
LEFT JOIN goals g ON t.goal_id = g.id
WHERE g.class_tags IS NOT NULL
GROUP BY t.user_id, UNNEST(g.class_tags);

CREATE UNIQUE INDEX ON missed_by_category (user_id, category);

-- Activity heatmap (daily task completion density)
CREATE MATERIALIZED VIEW activity_heatmap AS
SELECT
    user_id,
    DATE_TRUNC('day', scheduled_at) AS day,
    COUNT(*) FILTER (WHERE status = 'done') AS completed_count,
    COUNT(*) AS total_count
FROM tasks
WHERE scheduled_at IS NOT NULL
GROUP BY user_id, DATE_TRUNC('day', scheduled_at);

CREATE UNIQUE INDEX ON activity_heatmap (user_id, day);
```

### Migration 004 — Row Level Security Policies

```sql
-- Enable RLS on all user-data tables
ALTER TABLE users             ENABLE ROW LEVEL SECURITY;
ALTER TABLE goals             ENABLE ROW LEVEL SECURITY;
ALTER TABLE tasks             ENABLE ROW LEVEL SECURITY;
ALTER TABLE messages          ENABLE ROW LEVEL SECURITY;
ALTER TABLE conversations     ENABLE ROW LEVEL SECURITY;
ALTER TABLE patterns          ENABLE ROW LEVEL SECURITY;
ALTER TABLE notification_log  ENABLE ROW LEVEL SECURITY;
ALTER TABLE dispatch_log      ENABLE ROW LEVEL SECURITY;

-- users: Can only read/write own row
CREATE POLICY users_self ON users
    USING (id = auth.uid());

-- goals: Own goals only
CREATE POLICY goals_owner ON goals
    USING (user_id = auth.uid());

-- tasks: Own tasks only
CREATE POLICY tasks_owner ON tasks
    USING (user_id = auth.uid());

-- messages: Via conversation ownership
CREATE POLICY messages_owner ON messages
    USING (conversation_id IN (
        SELECT id FROM conversations WHERE user_id = auth.uid()
    ));

-- conversations: Own conversations only
CREATE POLICY conversations_owner ON conversations
    USING (user_id = auth.uid());

-- patterns: Own patterns only
CREATE POLICY patterns_owner ON patterns
    USING (user_id = auth.uid());

-- notification_log: Via task ownership
CREATE POLICY notification_log_owner ON notification_log
    USING (task_id IN (
        SELECT id FROM tasks WHERE user_id = auth.uid()
    ));

-- Service role bypasses RLS (used by backend workers)
-- The backend uses SUPABASE_SERVICE_ROLE_KEY for all server-side writes.
```

### Migration 005 — Triggers

```sql
-- Auto-refresh materialized views on task status update
CREATE OR REPLACE FUNCTION refresh_analytics_views()
RETURNS TRIGGER AS $$
BEGIN
    REFRESH MATERIALIZED VIEW CONCURRENTLY user_weekly_stats;
    REFRESH MATERIALIZED VIEW CONCURRENTLY missed_by_category;
    REFRESH MATERIALIZED VIEW CONCURRENTLY activity_heatmap;
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_refresh_analytics
    AFTER UPDATE OF status ON tasks
    FOR EACH STATEMENT
    EXECUTE FUNCTION refresh_analytics_views();

-- Auto-update patterns.updated_at
CREATE OR REPLACE FUNCTION update_patterns_timestamp()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = now();
    RETURN NEW;
END;
$$ LANGUAGE plpgsql;

CREATE TRIGGER trigger_patterns_updated_at
    BEFORE UPDATE ON patterns
    FOR EACH ROW
    EXECUTE FUNCTION update_patterns_timestamp();
```

### RRULE Expansion Strategy

The `recurrence_rule` column stores an iCal RRULE string (e.g., `RRULE:FREQ=WEEKLY;BYDAY=MO,WE,FR`). The system uses a **materialized** approach: the Scheduler writes one individual `tasks` row per occurrence when a goal is activated.

**Why materialized (not dynamic):** The Notifier polls individual task rows for notification dispatch. A dynamic approach would require on-the-fly occurrence generation inside the poll loop, creating complexity and inconsistency. Materialized rows are simpler, queryable, and indexable.

**Implementation in `app/services/rrule_expander.py`:**

```python
from dateutil.rrule import rrulestr
import pendulum

def expand_rrule_to_tasks(
    base_task: dict,
    rrule_string: str,
    start_dt: pendulum.DateTime,
    end_dt: pendulum.DateTime,
    user_timezone: str,
) -> list[dict]:
    """
    Expand a single RRULE into individual task dicts (one per occurrence).
    All scheduled_at values are returned as UTC.
    """
    rule = rrulestr(rrule_string, dtstart=start_dt)
    occurrences = rule.between(start_dt, end_dt, inc=True)
    tasks = []
    for occ in occurrences:
        # Convert local occurrence to UTC before storing
        local_dt = pendulum.instance(occ, tz=user_timezone)
        utc_dt = local_dt.in_timezone("UTC")
        task = {**base_task, "scheduled_at": utc_dt.isoformat(), "recurrence_rule": rrule_string}
        tasks.append(task)
    return tasks
```

---

## 6. Agent Architecture

There are **6 agents**. Each is a node (or subgraph) in the LangGraph execution graph. All agent JSON responses are validated against Pydantic models before being used downstream. Every LLM call is routed through the LiteLLM wrapper (§8) for fallback, retry, and cost tracking.

---

### 6.1 Orchestrator Agent

**Role:** Entry point for every user message. Classifies intent and routes to the correct downstream agent.

**LLM:** `openrouter/openai/gpt-4o` (fallback: `openrouter/anthropic/claude-sonnet-4-20250514`)

**Input:** Raw user message + conversation history (windowed — see §15) + user profile

**Output:** `OrchestratorOutput` Pydantic model

**Pydantic Model:**

```python
from pydantic import BaseModel
from typing import Literal, Optional

class OrchestratorOutput(BaseModel):
    intent: Literal["GOAL", "NEW_TASK", "RESCHEDULE_TASK", "MODIFY_GOAL", "CLARIFY"]
    payload: dict
    clarification_question: Optional[str] = None
    task_id: Optional[str] = None        # Present when intent == RESCHEDULE_TASK
    goal_id: Optional[str] = None        # Present when intent == MODIFY_GOAL
```

**System Prompt** (`app/agents/prompts/orchestrator.txt`):

```
You are the Orchestrator for Flux, a personal goal and task management assistant.

Analyze the user's message and classify it into exactly one of the following intents:
- GOAL: The user expresses an aspiration or multi-step desire requiring planning (e.g., "I want to lose weight", "I want to learn guitar")
- NEW_TASK: The user wants a discrete, one-time or recurring reminder (e.g., "Remind me to buy groceries at 5pm")
- RESCHEDULE_TASK: The user wants to reschedule a specific existing task. The message may contain a task_id.
- MODIFY_GOAL: The user wants to modify an existing active goal (e.g., change frequency, difficulty, or timing of a goal's tasks)
- CLARIFY: You need more information before routing (e.g., a task with no time or location context)

Rules:
- For NEW_TASK or RESCHEDULE_TASK: verify there is enough context (a specific time, a recurrence pattern, or a location trigger). If not, set intent to CLARIFY and specify exactly what is missing in clarification_question.
- For RESCHEDULE_TASK: extract task_id from the message if present (format: "task_id: <uuid>").
- For MODIFY_GOAL: extract goal_id from the message context if available.
- Never guess. If genuinely ambiguous, set intent to CLARIFY and ask one focused question.
- Respond ONLY with valid JSON matching this exact schema:
  {
    "intent": "GOAL" | "NEW_TASK" | "RESCHEDULE_TASK" | "MODIFY_GOAL" | "CLARIFY",
    "payload": {},
    "clarification_question": null | "string",
    "task_id": null | "uuid-string",
    "goal_id": null | "uuid-string"
  }
```

---

### 6.2 Goal Planning Agent

**Role:** Converts a validated goal statement into a concrete, sustainable 6-week action plan through multi-turn negotiation with the user.

**LLM:** `openrouter/anthropic/claude-sonnet-4-20250514` (fallback: `openrouter/openai/gpt-4o`)

**Input:** Goal statement + user profile + Classifier output + Scheduler output + Pattern Observer output (all three sub-agents run in parallel via LangGraph `Send()`)

**Output:** `GoalPlannerOutput` Pydantic model

**Pydantic Model:**

```python
from pydantic import BaseModel
from typing import Optional

class ProposedTask(BaseModel):
    title: str
    description: str
    scheduled_days: list[str]           # ["Monday", "Wednesday", "Friday"]
    suggested_time: str                 # "07:00" — in user local time
    duration_minutes: int
    recurrence_rule: str                # iCal RRULE string
    week_range: list[int]               # [1, 6]

class MicroGoal(BaseModel):
    title: str
    description: str
    pipeline_order: int
    target_weeks: int = 6

class ConflictDetected(BaseModel):
    existing_task_title: str
    scheduled_at: str
    message: str                        # Human-readable description of the conflict

class GoalPlannerOutput(BaseModel):
    goal_feasible_in_6_weeks: bool
    micro_goal_roadmap: Optional[list[MicroGoal]] = None
    proposed_tasks: list[ProposedTask]
    conflicts_detected: list[ConflictDetected] = []
    plan_summary: str                   # Human-readable plan to present to the user
    approval_status: str                # "pending" | "approved" | "negotiating" | "abandoned"
```

**Key Responsibilities:**

1. Assess if goal is achievable in ≤6 weeks. If not, decompose into ordered micro-goals. Queue all but the first micro-goal in `goals` with `status = 'pipeline'`.
2. Consult **Classifier**, **Scheduler**, and **Pattern Observer** in parallel (via `Send()` in LangGraph).
3. Detect overlap with existing active tasks; surface conflicts.
4. Generate proposed plan. Present to user as a structured card (returned as `plan_summary` + `proposed_tasks` list).
5. Enter negotiation loop until user explicitly approves. Only transition to `save_tasks` when `approval_status == "approved"`.
6. On confirmation: invoke Scheduler to write tasks to DB via `rrule_expander`.
7. If slots are insufficient for 6 weeks: set `goal.status = 'pipeline'` with a future `activated_at`.

**6-Week Rationale to Surface to User:**
> "Based on behavioral science research, humans maintain a >70% success rate when goals are structured as focused 6-week sprints. Your plan is designed around this principle."

**System Prompt** (`app/agents/prompts/goal_planner.txt`):

```
You are a world-class personal coach and behavioral scientist embedded in Flux, a goal management app.

Your task is to convert the user's goal into a concrete, achievable, sustainable 6-week action plan.

You have access to (passed in as context):
- user_profile: The user's schedule, sleep window, work hours, and chronotype
- classifier_output: Category tags for this goal
- scheduler_output: Available time slots for the next 6 weeks
- pattern_output: The user's behavioral history and avoidance patterns
- existing_tasks: The user's current active tasks

Principles:
1. SUSTAINABILITY: Never overload the user. Prefer fewer, consistent daily tasks over many sporadic ones.
2. INCREMENTALISM: Start easy. Ramp up gradually. Week 1 should feel almost too easy.
3. PATTERN AWARENESS: Avoid times the user has historically missed. Prefer high-completion times.
4. OVERLAP DETECTION: If an existing task covers something this goal needs, incorporate it rather than duplicating.
5. HONESTY: If slots are insufficient for 6 weeks, say so clearly and suggest a future start date.
6. NEGOTIATION: Always present the plan before saving. Allow the user to negotiate timing, frequency, or difficulty.
7. MICRO-GOALS: If the goal cannot be completed in 6 weeks, break it into ordered 6-week micro-goal sprints.

When generating tasks, output ONLY valid JSON matching the GoalPlannerOutput schema.
All times in suggested_time should be in the user's LOCAL timezone.
After presenting the plan, continue conversation to refine. Set approval_status to "approved" ONLY when the user explicitly accepts.
```

---

### 6.3 Classifier Agent

**Role:** Tags a goal with 1–3 category labels from a fixed taxonomy.

**LLM:** `openrouter/openai/gpt-4o-mini`

**Pydantic Model:**

```python
class ClassifierOutput(BaseModel):
    tags: list[str]   # 1–3 tags from the fixed taxonomy
```

**Fixed Taxonomy (v1):**
`Health`, `Fitness`, `Nutrition`, `Mental Health`, `Career`, `Learning`, `Financial`, `Relationships`, `Creativity`, `Productivity`, `Spirituality`, `Home`, `Travel`, `Other`

**System Prompt** (`app/agents/prompts/classifier.txt`):

```
You are a goal classifier. Given a user's goal statement, return 1–3 category tags.

Allowed tags (use ONLY from this list):
["Health", "Fitness", "Nutrition", "Mental Health", "Career", "Learning", "Financial",
 "Relationships", "Creativity", "Productivity", "Spirituality", "Home", "Travel", "Other"]

Rules:
- Never invent new tags.
- Prefer specific over general (e.g., "Fitness" not "Health" for exercise goals).
- Return ONLY valid JSON: { "tags": ["tag1", "tag2"] }
```

---

### 6.4 Scheduler Agent

**Role:** Manages the user's time. Finds available slots, detects conflicts, and writes confirmed task rows to the database via RRULE expansion.

**LLM:** `openrouter/openai/gpt-4o-mini`

**Pydantic Model:**

```python
class SlotResult(BaseModel):
    task_title: str
    scheduled_at: str       # ISO8601 UTC
    duration_minutes: int
    conflict: bool = False

class SchedulerOutput(BaseModel):
    slots: list[SlotResult]
    conflicts: list[dict]
    first_available_start: Optional[str] = None   # ISO8601 date if slots unavailable now
```

**Slot-Finding Logic:**

1. Load `users.profile.sleep_window` — block entirely.
2. Load `users.profile.work_hours` — soft block (lower preference, use only if no other slots).
3. Load all `tasks` with `status IN ('pending', 'rescheduled')` for the next 6 weeks.
4. Apply a 15-minute buffer between all tasks.
5. Use Pattern Observer hints to rank candidate slots by historical completion rate.
6. Return slots ranked by compatibility score. Never double-book.

**Timezone handling:** The Scheduler receives `suggested_time` in the user's local time. Before writing to the DB, convert all `scheduled_at` values to UTC using `pendulum` and the user's `timezone` field.

**System Prompt** (`app/agents/prompts/scheduler.txt`):

```
You are a scheduling assistant for Flux.

You have access to the user's availability (profile + existing tasks) and task requirements from the Goal Planner.

Find the best available time slots for the required tasks.

Availability rules:
- Never schedule during sleep_window
- Prefer not to schedule during work_hours unless no other slots exist
- Never double-book existing tasks
- Apply a 15-minute buffer between tasks
- Prefer times with historically high completion rates (from pattern_output)
- All times in suggested_time are in the user's LOCAL timezone (user_timezone provided in context)

Output ONLY valid JSON matching the SchedulerOutput schema.
```

---

### 6.5 Pattern Observer Agent

**Role:** Analyzes task completion and miss history to provide scheduling recommendations. Writes avoidance patterns to the `patterns` table.

**LLM:** `openrouter/openai/gpt-4o-mini`

**Pydantic Model:**

```python
class AvoidSlot(BaseModel):
    day: str
    time_range: str
    reason: str
    confidence: float

class CategoryPerformance(BaseModel):
    category: str
    completion_rate: float

class PatternObserverOutput(BaseModel):
    best_times: list[str]               # ["07:00–09:00", "18:00–19:30"]
    avoid_slots: list[AvoidSlot]
    category_performance: list[CategoryPerformance]
    general_notes: str
```

**Trigger Conditions:**

- **On consultation** (called by Goal Planner/Scheduler during goal creation): Summarize behavioral patterns as structured hints.
- **On task miss signal** (called by Notifier when `status = 'missed'`): Check if ≥3 consecutive misses in the same slot (±1 hour, same day of week) over 3 consecutive weeks. If so, create/update a `patterns` record with `pattern_type = 'time_avoidance'` and `confidence` proportional to miss frequency. **Do not overwrite patterns where `data.user_overridden = true`** — these have been manually corrected by the user via the Pattern Observer UI.

**Cold-Start Strategy (new users with no history):**

- Use `users.profile.chronotype` as baseline: `morning` → prefer 06:00–09:00 slots; `evening` → prefer 18:00–21:00.
- Apply research defaults: avoid Monday mornings for high-effort tasks; prefer consistent daily times over varied ones.
- Flag all patterns as `confidence < 0.5` for the first 2 weeks. After 14 days with task data, patterns become meaningful.

**System Prompt** (`app/agents/prompts/pattern_observer.txt`):

```
You are a behavioral pattern analyst for Flux.

Given a user's task history (completions and misses with timestamps), extract scheduling patterns.

Rules:
- Only report patterns with at least 3 data points.
- Mark patterns with fewer than 3 weeks of data as low-confidence (confidence < 0.5).
- Be specific about days and time ranges — avoid vague generalizations.

Output ONLY valid JSON matching the PatternObserverOutput schema.
```

---

### 6.6 Notifier Agent

**Role:** Background service that monitors task states and dispatches escalating notifications. Runs as a **separate process/container**.

**LLM:** None. Pure deterministic state machine.

**Escalation Chain:**

```
T = tasks.scheduled_at (UTC)

T - reminder_lead_minutes (default: 10)
  → Write dispatch_log row (status='pending', channel='push')
  → Atomic CAS update: UPDATE tasks SET reminder_sent_at=now()
      WHERE id=$1 AND reminder_sent_at IS NULL RETURNING id
  → If RETURNING returns a row: dispatch push notification
  → Update dispatch_log (status='dispatched')
  → Write notification_log row

Poll every 30s. After escalation_window_minutes (default: 2) with no response:

  → Write dispatch_log row (status='pending', channel='whatsapp')
  → Atomic CAS: UPDATE tasks SET whatsapp_sent_at=now()
      WHERE id=$1 AND whatsapp_sent_at IS NULL RETURNING id
  → If RETURNING returns a row: dispatch WhatsApp via Twilio
  → Store Twilio MessageSid in notification_log.external_id (idempotency key)

After another escalation_window_minutes with no response:

  → Write dispatch_log row (status='pending', channel='call')
  → Atomic CAS: UPDATE tasks SET call_sent_at=now()
      WHERE id=$1 AND call_sent_at IS NULL RETURNING id
  → If RETURNING returns a row: initiate Twilio voice call
  → Store Twilio CallSid in notification_log.external_id (idempotency key)

After another escalation_window_minutes with no response:

  → UPDATE tasks SET status='missed' WHERE id=$1 AND status='pending'
  → Notify Pattern Observer (async background call) if this is the 3rd consecutive miss
```

**Twilio Call Script (TTS):**

```
"Hi, this is your Flux assistant. Your task '[task_name]' is coming up.
Press 1 if you have already completed it.
Press 2 to reschedule.
Press 3 to mark it as missed."
```

DTMF responses: `1` → `status = 'done'`, `2` → `status = 'rescheduled'` (sends deep link to chat), `3` → `status = 'missed'`

**Reschedule from Notification:**

Before sending WhatsApp/call notifications, check for available slots in the remaining hours of the current day. If slots exist, include a reschedule deep link: `https://app.flux.com/chat?message=Reschedule+task+[task_id]:+[task_name]`. If no slots exist in the current day, omit the reschedule option.

**Location-Triggered Tasks (MVP — Demo Mode):**

Frontend POSTs to `POST /api/v1/demo/trigger-location`. Backend queries all `tasks` with `trigger_type = 'location'` and `status = 'pending'` for the user, then dispatches the normal notification chain. Poll every 2 hours if no acknowledgement.

**Startup Recovery:**

On worker startup, query `dispatch_log WHERE status = 'pending'` created more than 5 minutes ago. For each stuck record, re-attempt dispatch. This handles notification delivery failures caused by container restarts or crashes during dispatch.

```python
# notifier/recovery.py
async def recover_stuck_dispatches():
    cutoff = datetime.utcnow() - timedelta(minutes=5)
    stuck = await db.fetch(
        "SELECT * FROM dispatch_log WHERE status='pending' AND created_at < $1", cutoff
    )
    for record in stuck:
        await retry_dispatch(record)
```

---

## 7. LangGraph Orchestration

### Graph Structure

```python
# app/agents/graph.py
from langgraph.graph import StateGraph, END
from langgraph.types import Send
from app.agents.state import AgentState

graph = StateGraph(AgentState)

# Register all nodes
graph.add_node("orchestrator",      orchestrator_node)
graph.add_node("clarify",           clarify_node)
graph.add_node("onboarding",        onboarding_node)
graph.add_node("goal_planner",      goal_planner_node)
graph.add_node("classifier",        classifier_node)
graph.add_node("scheduler",         scheduler_node)
graph.add_node("pattern_observer",  pattern_observer_node)
graph.add_node("task_handler",      task_handler_node)
graph.add_node("goal_modifier",     goal_modifier_node)
graph.add_node("save_tasks",        save_tasks_node)

graph.set_entry_point("orchestrator")

# ── Orchestrator routing ──────────────────────────────────────────
graph.add_conditional_edges("orchestrator", route_from_orchestrator, {
    "ONBOARDING":       "onboarding",
    "GOAL":             "goal_planner",
    "NEW_TASK":         "task_handler",
    "RESCHEDULE_TASK":  "scheduler",
    "MODIFY_GOAL":      "goal_modifier",
    "CLARIFY":          "clarify",
})

graph.add_edge("clarify", "orchestrator")           # Loop back after clarification
graph.add_edge("onboarding", "orchestrator")         # After onboarding, re-route

# ── Goal planning fan-out (CORRECT: uses Send() for parallel execution) ──
# Architecture Review Critical Fix: Three add_edge() calls do NOT create parallel
# execution in LangGraph. Use Send() for true fan-out.
def fan_out_to_subagents(state: AgentState) -> list[Send]:
    return [
        Send("classifier",       {"goal_draft": state["goal_draft"],      "user_id": state["user_id"]}),
        Send("scheduler",        {"goal_draft": state["goal_draft"],      "user_profile": state["user_profile"]}),
        Send("pattern_observer", {"user_id": state["user_id"],            "user_profile": state["user_profile"]}),
    ]

graph.add_conditional_edges("goal_planner", fan_out_to_subagents)

# Sub-agents reconverge back to goal_planner for negotiation
graph.add_edge("classifier",       "goal_planner")
graph.add_edge("scheduler",        "goal_planner")
graph.add_edge("pattern_observer", "goal_planner")

graph.add_conditional_edges("goal_planner", check_user_approval, {
    "APPROVED":     "save_tasks",
    "NEGOTIATING":  "goal_planner",     # Continue conversation loop
    "ABANDONED":    END,
})

graph.add_edge("save_tasks",     END)
graph.add_edge("task_handler",   "save_tasks")
graph.add_edge("goal_modifier",  "save_tasks")
```

> **Implementation Note — `Send()` Reconvergence:** LangGraph's `Send()` dispatches sub-agent nodes in parallel, but reconvergence requires careful state merging. When `classifier`, `scheduler`, and `pattern_observer` each return partial state updates, the `goal_planner` node receives them via LangGraph's built-in reducer. Use the `Annotated` type with an `operator.add` or custom reducer on the relevant `AgentState` fields (`classifier_output`, `scheduler_output`, `pattern_output`) to ensure partial results are merged correctly without overwriting each other. If using a custom reducer, define it in `app/agents/state.py` alongside the `AgentState` TypedDict. Test this explicitly — incorrect reducer behavior can silently drop sub-agent results.

### State Schema

```python
# app/agents/state.py
from typing import TypedDict, Optional

class AgentState(TypedDict):
    user_id: str
    conversation_history: list[dict]    # Windowed — see §15 Cost Controls
    intent: Optional[str]
    user_profile: dict                  # Cached from DB at session start
    goal_draft: Optional[dict]
    proposed_tasks: Optional[list[dict]]
    classifier_output: Optional[dict]
    scheduler_output: Optional[dict]
    pattern_output: Optional[dict]
    approval_status: Optional[str]      # 'pending' | 'approved' | 'negotiating' | 'abandoned'
    error: Optional[str]
    token_usage: dict                   # Accumulated per-session token count
```

### Thread Persistence

- Each conversation session maps to a LangGraph thread identified by `langgraph_thread_id` stored in the `conversations` table.
- Thread state is persisted via `langgraph-checkpoint-postgres` backed by Supabase.
- **Critical:** Use the **direct connection string** (port 5432), NOT the PgBouncer pooler URL (port 6543). Transaction-mode pooling is incompatible with asyncpg and LangGraph's checkpoint writes.
- On session resume, load the thread and continue. Multi-turn goal negotiation can span multiple user sessions.

```python
# app/agents/graph.py — checkpointer setup
from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver

async def get_checkpointer():
    return AsyncPostgresSaver.from_conn_string(settings.database_url)
```

### LLM Output Validation & Retry Loop

Every agent LLM call is wrapped in a retry-with-correction loop. If the model returns malformed JSON, it re-prompts with the parse error and asks the model to fix its output. Maximum 2 retries before raising a graceful error.

```python
# app/services/llm.py
import json
from pydantic import BaseModel, ValidationError
from app.services.llm import llm_call    # LiteLLM wrapper

async def validated_llm_call(
    model: str,
    system_prompt: str,
    messages: list[dict],
    output_model: type[BaseModel],
    max_retries: int = 2,
) -> BaseModel:
    attempt = 0
    last_error = None
    while attempt <= max_retries:
        raw = await llm_call(model=model, system=system_prompt, messages=messages)
        try:
            parsed = json.loads(raw)
            return output_model(**parsed)
        except (json.JSONDecodeError, ValidationError) as e:
            last_error = str(e)
            messages = messages + [
                {"role": "assistant", "content": raw},
                {"role": "user", "content": f"Your response was invalid. Error: {last_error}. Please fix and return only valid JSON matching the schema."}
            ]
            attempt += 1
    raise ValueError(f"Agent failed to produce valid output after {max_retries} retries. Last error: {last_error}")
```

---

## 8. LLM Integration via LiteLLM + OpenRouter

All LLM calls are routed through **OpenRouter** via LiteLLM. OpenRouter acts as a unified gateway to OpenAI, Anthropic, and other providers — a single `OPENROUTER_API_KEY` covers all models, with unified billing and usage dashboards. LiteLLM handles the model string routing, fallback on outage, and retry logic.

### LiteLLM Configuration

```python
# app/services/llm.py
import litellm
from app.config import settings

litellm.set_verbose = False

# Point LiteLLM at OpenRouter for all calls
litellm.api_key = settings.openrouter_api_key
litellm.api_base = settings.openrouter_base_url

# Fallback configuration — OpenRouter model strings use openrouter/<provider>/<model> format
litellm.fallbacks = [
    {"openrouter/openai/gpt-4o":                          ["openrouter/anthropic/claude-sonnet-4-20250514"]},
    {"openrouter/anthropic/claude-sonnet-4-20250514":     ["openrouter/openai/gpt-4o"]},
    {"openrouter/openai/gpt-4o-mini":                     ["openrouter/anthropic/claude-haiku-4-5-20251001"]},
]

# Global retry settings
litellm.num_retries = 2
litellm.request_timeout = 30   # seconds

async def llm_call(
    model: str,
    system: str,
    messages: list[dict],
    max_tokens: int = 2048,
    user_id: str = None,
) -> str:
    """
    Unified LLM call via LiteLLM → OpenRouter. Returns raw text response.
    Tracks token usage and writes to users.monthly_token_usage.
    """
    full_messages = [{"role": "system", "content": system}] + messages

    response = await litellm.acompletion(
        model=model,
        messages=full_messages,
        max_tokens=max_tokens,
        extra_headers={
            # OpenRouter attribution headers (optional but recommended)
            "HTTP-Referer": settings.openrouter_app_url,
            "X-Title": settings.openrouter_app_name,
        },
        # LangSmith tracing is automatic when LANGCHAIN_TRACING_V2=true
    )

    # Track token usage for cost controls (all go through OpenRouter)
    if user_id and response.usage:
        await update_token_usage(
            user_id=user_id,
            provider="openrouter",
            tokens=response.usage.total_tokens,
        )

    return response.choices[0].message.content
```

### Token Usage Tracking

```python
# app/services/llm.py
from app.services.supabase import db

async def update_token_usage(user_id: str, provider: str, tokens: int):
    """Atomically increment monthly token usage for a user."""
    await db.execute("""
        UPDATE users SET monthly_token_usage = jsonb_set(
            jsonb_set(
                monthly_token_usage,
                ARRAY[$1],
                (COALESCE((monthly_token_usage->$1)::int, 0) + $2)::text::jsonb
            ),
            ARRAY['total'],
            (COALESCE((monthly_token_usage->'total')::int, 0) + $2)::text::jsonb
        )
        WHERE id = $3
    """, provider, tokens, user_id)

async def check_token_budget(user_id: str) -> str:
    """Returns 'ok' | 'soft_limit' | 'hard_limit'"""
    row = await db.fetchrow("SELECT monthly_token_usage FROM users WHERE id = $1", user_id)
    total = row["monthly_token_usage"].get("total", 0)
    if total >= settings.monthly_token_hard_limit:
        return "hard_limit"
    if total >= settings.monthly_token_soft_limit:
        return "soft_limit"
    return "ok"
```

---

## 9. Notification Engine

### Architecture

The Notifier runs as a **separate Docker container** sharing the same Supabase database. It uses **APScheduler with a SQLAlchemy PostgreSQL job store** — this persists scheduled jobs across container restarts, preventing silent notification drops on redeploy.

```python
# notifier/main.py
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
from notifier.poll import notification_poll
from notifier.recovery import recover_stuck_dispatches
from app.config import settings
import asyncio

jobstores = {
    "default": SQLAlchemyJobStore(url=settings.database_url.replace("+asyncpg", ""))
}

scheduler = AsyncIOScheduler(jobstores=jobstores)

async def main():
    # On startup: recover any stuck pending dispatches
    await recover_stuck_dispatches()

    scheduler.add_job(
        notification_poll,
        "interval",
        seconds=settings.notification_poll_interval_seconds,
        id="notification_poll",
        replace_existing=True,
    )
    scheduler.start()
    await asyncio.Event().wait()    # Keep alive

if __name__ == "__main__":
    asyncio.run(main())
```

### Core Poll Loop

```python
# notifier/poll.py
from datetime import datetime, timedelta
from app.config import settings
from app.services.supabase import db
from notifier.dispatch import dispatch_push, dispatch_whatsapp, dispatch_call

async def notification_poll():
    now = datetime.utcnow()
    lead = timedelta(minutes=settings.reminder_lead_minutes)
    window = timedelta(minutes=settings.escalation_window_minutes)

    # ── PUSH: due in next reminder_lead_minutes, not yet notified ──
    due_push = await db.fetch("""
        SELECT id, user_id, title, scheduled_at
        FROM tasks
        WHERE status = 'pending'
          AND trigger_type = 'time'
          AND scheduled_at BETWEEN $1 AND $2
          AND reminder_sent_at IS NULL
    """, now, now + lead)

    for task in due_push:
        # Atomic CAS to prevent double-fire in concurrent workers
        claimed = await db.fetchrow("""
            UPDATE tasks SET reminder_sent_at = now()
            WHERE id = $1 AND reminder_sent_at IS NULL
            RETURNING id
        """, task["id"])
        if claimed:
            await log_dispatch(task["id"], "push")
            await dispatch_push(task)
            await mark_dispatch_done(task["id"], "push")

    # ── WHATSAPP: push sent > escalation_window ago, no response ──
    due_whatsapp = await db.fetch("""
        SELECT id, user_id, title, scheduled_at
        FROM tasks
        WHERE status = 'pending'
          AND reminder_sent_at IS NOT NULL
          AND reminder_sent_at <= $1
          AND whatsapp_sent_at IS NULL
    """, now - window)

    for task in due_whatsapp:
        claimed = await db.fetchrow("""
            UPDATE tasks SET whatsapp_sent_at = now()
            WHERE id = $1 AND whatsapp_sent_at IS NULL
            RETURNING id
        """, task["id"])
        if claimed:
            await log_dispatch(task["id"], "whatsapp")
            message_sid = await dispatch_whatsapp(task)
            await mark_dispatch_done(task["id"], "whatsapp", external_id=message_sid)

    # ── CALL: whatsapp sent > escalation_window ago, no response ──
    due_call = await db.fetch("""
        SELECT id, user_id, title, scheduled_at
        FROM tasks
        WHERE status = 'pending'
          AND whatsapp_sent_at IS NOT NULL
          AND whatsapp_sent_at <= $1
          AND call_sent_at IS NULL
    """, now - window)

    for task in due_call:
        claimed = await db.fetchrow("""
            UPDATE tasks SET call_sent_at = now()
            WHERE id = $1 AND call_sent_at IS NULL
            RETURNING id
        """, task["id"])
        if claimed:
            await log_dispatch(task["id"], "call")
            call_sid = await dispatch_call(task)
            await mark_dispatch_done(task["id"], "call", external_id=call_sid)

    # ── AUTO-MISS: call sent > escalation_window ago, no response ──
    due_miss = await db.fetch("""
        SELECT id, user_id
        FROM tasks
        WHERE status = 'pending'
          AND call_sent_at IS NOT NULL
          AND call_sent_at <= $1
    """, now - window)

    for task in due_miss:
        await db.execute("""
            UPDATE tasks SET status = 'missed'
            WHERE id = $1 AND status = 'pending'
        """, task["id"])
        await check_and_flag_pattern(task["user_id"], task["id"])
```

### Twilio Dispatch

```python
# notifier/dispatch.py
from twilio.rest import Client
from app.config import settings

twilio = Client(settings.twilio_account_sid, settings.twilio_auth_token)

async def dispatch_whatsapp(task: dict) -> str:
    """Returns MessageSid for idempotency tracking."""
    user = await get_user_with_phone(task["user_id"])
    if not user["phone_verified"] or not user["whatsapp_opt_in_at"]:
        return None

    reschedule_available = await check_remaining_day_slots(task["user_id"])
    reschedule_line = "\n2️⃣ Reschedule" if reschedule_available else ""

    message = twilio.messages.create(
        from_=settings.twilio_whatsapp_from,
        to=f"whatsapp:{user['notification_preferences']['phone_number']}",
        body=(
            f"Hi! Your task *{task['title']}* is coming up. Please respond:\n"
            f"1️⃣ Already done"
            f"{reschedule_line}\n"
            f"3️⃣ Mark as missed"
        )
    )
    return message.sid

async def dispatch_call(task: dict) -> str:
    """Returns CallSid for idempotency tracking."""
    user = await get_user_with_phone(task["user_id"])
    if not user["phone_verified"]:
        return None

    callback_url = f"{settings.twilio_webhook_base_url}/api/v1/webhooks/twilio/voice?task_id={task['id']}"

    twiml = f"""<?xml version="1.0" encoding="UTF-8"?>
<Response>
    <Gather numDigits="1" action="{callback_url}" method="POST">
        <Say>Hi! This is your Flux assistant. Your task {task['title']} is coming up.
        Press 1 if you have already completed it.
        Press 2 to reschedule.
        Press 3 to mark it as missed.</Say>
    </Gather>
</Response>"""

    call = twilio.calls.create(
        from_=settings.twilio_voice_from,
        to=user["notification_preferences"]["phone_number"],
        twiml=twiml,
    )
    return call.sid
```

### Web Push Dispatch

```python
# app/services/push_service.py
from pywebpush import webpush, WebPushException
from app.config import settings
import json

async def dispatch_push(task: dict, user_push_subscription: dict):
    """Send a Web Push notification."""
    if not user_push_subscription:
        return
    try:
        webpush(
            subscription_info=user_push_subscription,
            data=json.dumps({
                "title": "🔔 Task Reminder",
                "body": f"{task['title']} coming up!",
                "task_id": str(task["id"]),
                "actions": [
                    {"action": "done",     "title": "Already Done"},
                    {"action": "reschedule", "title": "Reschedule"},
                    {"action": "missed",   "title": "Mark Missed"},
                ]
            }),
            vapid_private_key=settings.vapid_private_key,
            vapid_claims={"sub": settings.vapid_claims_email},
        )
    except WebPushException as e:
        # Log but don't raise — push subscriptions can expire
        logger.warning("push_failed", task_id=str(task["id"]), error=str(e))
```

---

## 10. Analytics

All analytics are computed from the `tasks` table via materialized views (refreshed by trigger on task status update — see Migration 005). The FastAPI analytics endpoints query these views directly.

### KPIs

| KPI | Materialized View / Query | Frontend Visualization |
|---|---|---|
| Goal Progress | Direct query on `tasks` per goal | Progress bar + % |
| Weekly Completion % | `user_weekly_stats` | Line chart (12-week rolling) |
| Missed Tasks by Category | `missed_by_category` | Bar chart |
| Activity Heatmap | `activity_heatmap` | GitHub-style calendar heatmap |
| Streak Tracker | Computed from `activity_heatmap` | Flame icon + count |

### Streak Calculation

```sql
-- Current streak: consecutive days with ≥1 completed task, counting back from today
WITH daily AS (
    SELECT day, completed_count > 0 AS had_completion
    FROM activity_heatmap
    WHERE user_id = $1 AND day <= CURRENT_DATE
    ORDER BY day DESC
),
streak AS (
    SELECT day, had_completion,
           ROW_NUMBER() OVER (ORDER BY day DESC) AS rn,
           ROW_NUMBER() OVER (PARTITION BY had_completion ORDER BY day DESC) AS grp
    FROM daily
)
SELECT COUNT(*) AS streak_days
FROM streak
WHERE had_completion = true AND rn = grp;
```

---

## 11. API Design — OpenAPI / Swagger

FastAPI auto-generates OpenAPI 3.0 documentation. Access at:
- **Swagger UI:** `GET /docs`
- **ReDoc:** `GET /redoc`
- **OpenAPI JSON schema:** `GET /openapi.json`

### FastAPI App Setup

```python
# app/main.py
import sentry_sdk
from sentry_sdk.integrations.fastapi import FastApiIntegration
from sentry_sdk.integrations.asyncio import AsyncioIntegration
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import structlog

from app.config import settings
from app.api.v1 import chat, goals, tasks, analytics, patterns, account, demo, webhooks
from app.middleware.logging import StructlogMiddleware

# ── Sentry Initialization (must be before FastAPI app creation) ──
sentry_sdk.init(
    dsn=settings.sentry_dsn,
    traces_sample_rate=settings.sentry_traces_sample_rate,
    environment=settings.sentry_environment,
    integrations=[FastApiIntegration(), AsyncioIntegration()],
    send_default_pii=False,    # GDPR: no PII in error payloads
)

# ── Rate Limiter ──
limiter = Limiter(key_func=get_remote_address, storage_uri=settings.redis_url)

app = FastAPI(
    title="Flux API",
    description="AI-powered goal and task management backend",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_url="/openapi.json",
)

app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
app.add_middleware(StructlogMiddleware)
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_methods=["*"], allow_headers=["*"])

# ── Routers ──
app.include_router(chat.router,      prefix="/api/v1", tags=["Chat"])
app.include_router(goals.router,     prefix="/api/v1", tags=["Goals"])
app.include_router(tasks.router,     prefix="/api/v1", tags=["Tasks"])
app.include_router(analytics.router, prefix="/api/v1", tags=["Analytics"])
app.include_router(patterns.router,  prefix="/api/v1", tags=["Patterns"])
app.include_router(account.router,   prefix="/api/v1", tags=["Account"])
app.include_router(demo.router,      prefix="/api/v1", tags=["Demo"])
app.include_router(webhooks.router,  prefix="/api/v1", tags=["Webhooks"])
```

### All API Endpoints

All endpoints (except webhooks) require a valid Supabase JWT in `Authorization: Bearer <token>`. The JWT is validated on every request by the `auth` dependency.

```python
# app/dependencies.py
from fastapi import Depends, HTTPException, Security
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from supabase import create_client

security = HTTPBearer()

async def get_current_user(credentials: HTTPAuthorizationCredentials = Security(security)):
    """Validate Supabase JWT and return user_id."""
    try:
        supabase = create_client(settings.supabase_url, settings.supabase_anon_key)
        user = supabase.auth.get_user(credentials.credentials)
        return user.user
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid or expired token")
```

---

#### Chat Endpoints

```
POST   /api/v1/chat/message
```

Request body:
```json
{
  "message": "I want to run a 5K in 6 weeks",
  "conversation_id": "uuid | null"
}
```

Response (streaming via `StreamingResponse`):
```json
{
  "conversation_id": "uuid",
  "message": "Great goal! Let me put together a plan...",
  "agent_node": "goal_planner",
  "proposed_plan": { ... } | null,
  "requires_user_action": "approve_plan | none"
}
```

Rate limit: **20 requests/minute per user**

```
GET    /api/v1/chat/history?conversation_id=<uuid>&limit=50
```

Response:
```json
{
  "messages": [
    {
      "id": "uuid",
      "role": "user | assistant | summary",
      "content": "...",
      "agent_node": "goal_planner | null",
      "created_at": "ISO8601"
    }
  ]
}
```

---

#### Goal Endpoints

```
GET    /api/v1/goals?status=active|pipeline|completed|abandoned
```

```
GET    /api/v1/goals/{goal_id}
```

Response includes `pipeline` array of micro-goals if applicable.

```
PATCH  /api/v1/goals/{goal_id}/abandon
```

Cancels all future `pending` tasks for this goal (that are not `shared_with_goal_ids` containing another active goal). Sets goal `status = 'abandoned'`.

```
PATCH  /api/v1/goals/{goal_id}/modify
```

Triggers `MODIFY_GOAL` intent in LangGraph. Body:
```json
{ "message": "Change my gym sessions from 3x to 2x per week" }
```

This opens a negotiation flow via the Goal Modifier node: identifies affected tasks, cancels future occurrences, generates replacement tasks, re-runs conflict detection, presents to user for approval, then writes updated tasks.

```
GET    /api/v1/goals/{goal_id}/tasks
```

Returns all tasks for a given goal.

---

#### Task Endpoints

```
GET    /api/v1/tasks/today
```

Returns all `pending` tasks for the current day in user's local time, ordered by `scheduled_at`.

```
GET    /api/v1/tasks/{task_id}
```

```
PATCH  /api/v1/tasks/{task_id}/complete
```

Sets `status = 'done'`, `completed_at = now()`. Triggers micro-goal pipeline check (if the completed task was the last task of the current sprint, activate the next pipeline goal).

```
PATCH  /api/v1/tasks/{task_id}/missed
```

Sets `status = 'missed'`. Triggers Pattern Observer miss signal asynchronously.

```
POST   /api/v1/tasks/{task_id}/reschedule
```

Body: `{ "message": "Move this to tomorrow at 8am" }`. Invokes Scheduler via LangGraph for slot-finding. Returns proposed new `scheduled_at` for user confirmation.

---

#### Analytics Endpoints

```
GET    /api/v1/analytics/overview
```

Response:
```json
{
  "streak_days": 7,
  "today_completion_pct": 66.7,
  "today_done": 2,
  "today_total": 3,
  "heatmap": [{ "day": "2026-02-01", "completed": 3, "total": 4 }]
}
```

```
GET    /api/v1/analytics/goals
```

Per-goal progress bars and category breakdown.

```
GET    /api/v1/analytics/missed-by-cat
```

Missed tasks grouped by goal category.

```
GET    /api/v1/analytics/weekly?weeks=12
```

Rolling N-week completion rate from `user_weekly_stats`.

---

#### Pattern Endpoints (MVP — Pattern Observer UI)

These endpoints expose the user's learned behavioral patterns so they can be viewed, corrected, and deleted via the Analytics screen. No new database tables are required — the `patterns` table (Migration 001) and its RLS policy (Migration 004) already support this fully.

```
GET    /api/v1/patterns
```

Returns all patterns for the authenticated user, ordered by `updated_at DESC`.

Response:
```json
{
  "patterns": [
    {
      "id": "uuid",
      "pattern_type": "time_avoidance",
      "description": "You consistently miss tasks on Monday mornings between 07:00–09:00",
      "data": { "day": "Monday", "time_range": "07:00–09:00", "miss_count": 4 },
      "confidence": 0.85,
      "created_at": "ISO8601",
      "updated_at": "ISO8601"
    }
  ]
}
```

```
GET    /api/v1/patterns/{pattern_id}
```

Returns a single pattern by ID.

```
PATCH  /api/v1/patterns/{pattern_id}
```

Allows the user to **override or correct** a pattern. Use cases: mark a pattern as inaccurate ("I actually like Monday mornings now"), adjust confidence, or update the description.

Request body:
```json
{
  "user_override": true,
  "description": "This is no longer accurate — I've changed my schedule",
  "confidence": 0.1
}
```

Setting `confidence` to a low value (≤0.1) effectively suppresses the pattern from influencing scheduling recommendations. The Pattern Observer will not overwrite overridden patterns unless new miss data resets them (tracked via the `data.user_overridden` flag).

The PATCH writes the following to the `patterns` row:
- `confidence` updated to supplied value
- `description` updated if provided
- `data.user_overridden = true` set in JSONB to prevent the Pattern Observer from overwriting it automatically

```
DELETE /api/v1/patterns/{pattern_id}
```

Hard-deletes the pattern. The Pattern Observer may re-detect and re-create it if the behavior recurs. Returns `HTTP 204 No Content`.

---

#### Account Endpoints

```
GET    /api/v1/account/me
```

Returns user profile, notification preferences, and monthly token usage summary.

```
PATCH  /api/v1/account/me
```

Update notification preferences (reminder lead time, escalation window).

```
POST   /api/v1/account/phone/verify/send
```

Sends OTP via Twilio Verify. Body: `{ "phone_number": "+971501234567" }`.

```
POST   /api/v1/account/phone/verify/confirm
```

Confirms OTP and sets `phone_verified = true`. Body: `{ "phone_number": "+971501234567", "code": "123456" }`.

```
POST   /api/v1/account/whatsapp/opt-in
```

Sets `whatsapp_opt_in_at = now()`. Requires `phone_verified = true`.

```
DELETE /api/v1/account
```

**GDPR Right to Erasure.** Cascading deletion:
1. Delete all rows in: `tasks`, `goals`, `patterns`, `messages`, `conversations`, `notification_log`, `dispatch_log`.
2. Delete LangGraph checkpoint threads for this user.
3. Delete `users` row.
4. Queue async jobs to request data deletion via OpenRouter's data deletion process. Note: OpenRouter routes calls to OpenAI and Anthropic — also contact those providers directly per their individual data retention policies if needed.

```
GET    /api/v1/account/export
```

**GDPR Right to Data Portability.** Returns a JSON ZIP of all user data: profile, goals, tasks, messages, patterns.

---

#### Webhook Endpoints (Twilio)

Webhook endpoints do **not** require Supabase JWT auth. Instead, validate the **Twilio request signature** on every incoming webhook.

```python
# Twilio signature validation middleware
from twilio.request_validator import RequestValidator

def validate_twilio_signature(request: Request):
    validator = RequestValidator(settings.twilio_auth_token)
    signature = request.headers.get("X-Twilio-Signature", "")
    url = str(request.url)
    params = dict(await request.form())
    if not validator.validate(url, params, signature):
        raise HTTPException(status_code=403, detail="Invalid Twilio signature")
```

```
POST   /api/v1/webhooks/twilio/whatsapp
```

Handles WhatsApp replies. Maps `Body` to task action:
- `"1"` or `"done"` → `PATCH /tasks/{task_id}/complete`
- `"2"` or `"reschedule"` → generate reschedule deep link
- `"3"` or `"missed"` → `PATCH /tasks/{task_id}/missed`

Uses Twilio `MessageSid` as idempotency key (`notification_log.external_id`) to prevent duplicate processing on Twilio retries.

```
POST   /api/v1/webhooks/twilio/voice
```

Handles DTMF responses from voice calls. Query param: `?task_id=<uuid>`. Maps DTMF digits identically to WhatsApp actions above. Uses `CallSid` as idempotency key.

---

#### Demo Mode Endpoints

```
POST   /api/v1/demo/trigger-location
```

Simulates "I'm away from home" GPS trigger. Queries all `pending` tasks with `trigger_type = 'location'` for the authenticated user and fires the notification chain.

---

### OpenAPI Schema Customization

```python
# app/main.py — Add to FastAPI() constructor
from fastapi.openapi.utils import get_openapi

def custom_openapi():
    if app.openapi_schema:
        return app.openapi_schema
    schema = get_openapi(
        title="Flux API",
        version="1.0.0",
        description="AI-powered goal and task management. All endpoints require Bearer JWT except webhooks.",
        routes=app.routes,
    )
    # Add global security scheme
    schema["components"]["securitySchemes"] = {
        "BearerAuth": {"type": "http", "scheme": "bearer", "bearerFormat": "JWT"}
    }
    schema["security"] = [{"BearerAuth": []}]
    app.openapi_schema = schema
    return schema

app.openapi = custom_openapi
```

---

## 12. Onboarding Flow

Onboarding is conducted entirely via the chat interface. The Orchestrator detects `users.onboarded = false` on the user's first message and routes all requests to the `onboarding` LangGraph subgraph until onboarding completes.

### Onboarding Questions (Conversational Order)

1. **Name** — "What should I call you?"
2. **Wake time** — "What time do you usually wake up?"
3. **Sleep time** — "What time do you go to bed?"
4. **Work schedule** — "Do you work during the day? If so, roughly what hours?"
5. **Chronotype** — "Are you more of a morning person or a night owl?"
6. **Timezone** — Captured automatically from the frontend (`Intl.DateTimeFormat().resolvedOptions().timeZone`). Confirm with user: "I see you're in [timezone] — is that right?"
7. **Location labels** — "What should I call your home location?" (for demo mode and future GPS)
8. **Existing commitments** — "Any existing regular commitments I should know about? (e.g., gym on Tuesday evenings)"
9. **Phone number** — "What's your phone number? I'll use it to send you reminders if you don't respond in the app." → Trigger OTP verification flow.
10. **WhatsApp opt-in** — "Can I also reach you on WhatsApp at this number?" (explicit consent, stored with timestamp)
11. **First goal or task** — "Great, [name]! Now, what's the first thing you'd like to work on?" → Transitions into normal goal/task flow.

### Profile JSON Written to `users.profile`

```json
{
  "name": "Alex",
  "sleep_window": { "start": "23:00", "end": "07:00" },
  "work_hours": { "start": "09:00", "end": "18:00", "days": ["Mon","Tue","Wed","Thu","Fri"] },
  "chronotype": "morning",
  "existing_commitments": [
    { "title": "Gym", "days": ["Tuesday"], "time": "19:00", "duration_minutes": 60 }
  ],
  "locations": { "home": "Home", "work": "Office" }
}
```

After onboarding: set `users.onboarded = true`, `users.timezone = <captured timezone>`, and pre-seed `existing_commitments` as `tasks` rows in the database.

---

## 13. Privacy, Security & Compliance

### Row Level Security

All user-data tables have RLS enabled (see Migration 004). The backend uses `SUPABASE_SERVICE_ROLE_KEY` for all server-side operations (which bypasses RLS). The Supabase anon key is only used for validating incoming JWTs from the client.

### JWT Validation

All API endpoints validate the Supabase JWT. JWTs are short-lived (1 hour) with refresh tokens managed by the Supabase client library on the frontend.

### PII Handling in LLM Prompts

Only `user_id` (UUID) is passed between agents in internal payloads — never names, email addresses, or phone numbers. User profile data referenced in prompts is passed as structured JSON, not as raw prose with identifying information.

### Twilio Webhook Signature Validation

Every Twilio webhook request is validated using `twilio.request_validator.RequestValidator` and the `TWILIO_AUTH_TOKEN`. Requests failing validation are rejected with `HTTP 403`. This prevents spoofed webhook attacks.

### Phone Number Verification

Phone numbers are verified via Twilio Verify (OTP) during onboarding. All WhatsApp and voice notifications are gated on `users.phone_verified = true`. WhatsApp dispatch is additionally gated on `users.whatsapp_opt_in_at IS NOT NULL` (double opt-in per WhatsApp Business API compliance requirements).

### WhatsApp — Twilio Sandbox (MVP)

> **Sandbox-Only for MVP:** For the initial build and all local/staging testing, use the **Twilio WhatsApp Sandbox**. The sandbox requires each recipient to send a join code (`join <sandbox-keyword>`) before receiving messages — this is a Twilio limitation, not a product choice. Meta Business API approval (2–4 weeks) is deferred to a later phase and is **not** a launch blocker for MVP. The notification engine should be designed to work identically with both sandbox and production WhatsApp — the only difference is the `TWILIO_WHATSAPP_FROM` number.

### Data Minimization

Pattern Observer stores only aggregated pattern summaries in the `patterns` table — not raw behavioral logs. Raw data lives only in the `tasks` table (which the user can export or delete).

### GDPR Compliance

- **Right to Erasure:** `DELETE /api/v1/account` endpoint (see §11) cascades all user data and queues provider deletion requests.
- **Right to Portability:** `GET /api/v1/account/export` returns a complete data export as JSON ZIP.
- **Data at Rest:** All Supabase data is encrypted at rest (AES-256) by default.
- **No Third-Party Analytics SDKs:** All analytics are computed from first-party Supabase data. No tracking pixels, analytics SDKs, or ad networks.

---

## 14. Observability — LangSmith, Sentry, Structured Logging

### LangSmith (Agent Tracing)

LangSmith is enabled by setting `LANGCHAIN_TRACING_V2=true`. This single environment variable enables automatic capture of every LangGraph node execution, every LLM call (model, prompt, response, tokens, latency), and every agent error. No code changes required beyond the env var.

Access traces at: `https://smith.langchain.com/projects/flux-production`

**What to monitor:**
- Agent node success/failure rates per node type
- LLM call latency (p50, p95, p99) per model
- Token usage per session and per agent
- Goal planning negotiation turn counts
- Classifier tag distribution

### Sentry (Error & Performance Tracking)

Sentry is initialized before FastAPI app creation (see §11). It captures:
- All unhandled exceptions in FastAPI routes
- All unhandled exceptions in the Notifier worker
- Async task errors via `AsyncioIntegration`
- Performance traces (20% sampling rate in production)

**`send_default_pii=False`** — ensures no user PII is included in Sentry error payloads (GDPR).

**Alert Rules to Configure in Sentry:**
- LLM error rate (any agent) > 5% in 5-minute window → page on-call
- Notification dispatch failure rate > 5% → page on-call
- API p95 latency > 10 seconds → page on-call
- Any unhandled exception in the Notifier worker → page on-call

### Structured Logging with structlog

```python
# app/middleware/logging.py
import structlog
import uuid
from starlette.middleware.base import BaseHTTPMiddleware

structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.stdlib.add_log_level,
        structlog.processors.JSONRenderer(),
    ]
)

logger = structlog.get_logger()

class StructlogMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        correlation_id = str(uuid.uuid4())
        request.state.correlation_id = correlation_id
        with structlog.contextvars.bound_contextvars(
            correlation_id=correlation_id,
            method=request.method,
            path=request.url.path,
        ):
            response = await call_next(request)
            logger.info("request_completed", status_code=response.status_code)
            response.headers["X-Correlation-ID"] = correlation_id
            return response
```

Every log line is JSON-formatted and includes `correlation_id`, tying a user session to all agent calls, LLM invocations, and DB writes. The `correlation_id` should also be passed through LangGraph's `AgentState` for end-to-end trace correlation.

---

## 15. Cost Controls

### Conversation Context Window Management

`AgentState.conversation_history` does not grow unboundedly. When the history exceeds `MAX_CONVERSATION_MESSAGES` (20) or `MAX_CONVERSATION_TOKENS` (8,000), older messages are summarized using GPT-4o-mini and replaced with a single summary message.

```python
# app/services/context_manager.py
from app.services.llm import llm_call
from app.config import settings

SUMMARIZE_PROMPT = """
You are summarizing an older portion of a conversation between a user and their AI assistant.
Create a concise factual summary of the key information, decisions, and context established
in this conversation segment. The summary will replace the original messages.
Return only the summary text, no preamble.
"""

async def window_conversation_history(
    history: list[dict],
    user_id: str,
) -> list[dict]:
    """
    If conversation history exceeds limits, summarize the oldest portion.
    Returns the windowed history ready for the next LLM call.
    """
    if len(history) <= settings.max_conversation_messages:
        return history

    # Split: summarize the older half, keep the recent half verbatim
    split_idx = len(history) // 2
    older = history[:split_idx]
    recent = history[split_idx:]

    summary_text = await llm_call(
        model="openrouter/openai/gpt-4o-mini",
        system=SUMMARIZE_PROMPT,
        messages=older,
        max_tokens=500,
        user_id=user_id,
    )

    summary_message = {
        "role": "summary",
        "content": f"[Earlier conversation summary]: {summary_text}"
    }

    return [summary_message] + recent
```

This summary is also written to the `messages` table with `role = 'summary'` for auditability.

### Per-User Monthly Budgets

When `check_token_budget()` returns `"soft_limit"`, a warning is shown to the user in the chat UI ("You're approaching your monthly usage limit"). When it returns `"hard_limit"`, the Orchestrator and Goal Planner are downgraded to `openrouter/openai/gpt-4o-mini` to reduce spend. Budget resets monthly via a scheduled cron job that updates `monthly_token_usage.reset_at`.

### `max_tokens` on Every LLM Call

Every `llm_call()` invocation must specify `max_tokens`. Defaults by agent:

| Agent | `max_tokens` |
|---|---|
| Orchestrator | 512 |
| Goal Planner | 4096 |
| Classifier | 128 |
| Scheduler | 1024 |
| Pattern Observer | 1024 |
| Summarizer | 500 |

### Provider Hard Spend Caps

Since all LLM calls route through OpenRouter, set a single spend limit in the **OpenRouter billing dashboard** as a last-resort backstop. This covers all underlying providers (OpenAI, Anthropic, etc.) in one place. Set both a soft alert and a hard monthly limit at: `https://openrouter.ai/settings/billing`.

---

## 16. Rate Limiting

Rate limiting is implemented via `slowapi`, backed by Redis. Limits are applied per authenticated user (by user ID, not IP).

```python
# app/middleware/rate_limit.py
from slowapi import Limiter
from app.config import settings

limiter = Limiter(
    key_func=lambda request: request.state.user.id,    # Per-user limiting
    storage_uri=settings.redis_url,
)
```

### Limits

| Endpoint | Limit |
|---|---|
| `POST /chat/message` | 20 requests/minute |
| `POST /goals` (via chat) | 5 per hour |
| `POST /tasks` (via chat) | 60 per hour |
| `GET /analytics/*` | 30 per minute |
| `POST /account/phone/verify/send` | 3 per hour |
| All other endpoints | 100 per minute |

Rate limit responses return `HTTP 429` with a `Retry-After` header. The chat UI should surface a friendly message when rate-limited.

---

## 17. Docker & Deployment

### Dockerfile — API

```dockerfile
# docker/Dockerfile.api
FROM python:3.12-slim

WORKDIR /app

# Install uv
COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

# Copy dependency files
COPY pyproject.toml uv.lock ./

# Install dependencies (no dev deps in production)
RUN uv sync --frozen --no-dev

# Copy application code
COPY app/ ./app/

EXPOSE 8000

CMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--workers", "4"]
```

### Dockerfile — Notifier Worker

```dockerfile
# docker/Dockerfile.notifier
FROM python:3.12-slim

WORKDIR /app

COPY --from=ghcr.io/astral-sh/uv:latest /uv /usr/local/bin/uv

COPY pyproject.toml uv.lock ./
RUN uv sync --frozen --no-dev

COPY app/ ./app/
COPY notifier/ ./notifier/

CMD ["uv", "run", "python", "-m", "notifier.main"]
```

### docker-compose.yml (Local Development)

```yaml
version: "3.9"

services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    ports:
      - "8000:8000"
    env_file: .env
    depends_on:
      - redis
    volumes:
      - ./app:/app/app      # Hot reload in development
    command: ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000", "--reload"]

  notifier:
    build:
      context: .
      dockerfile: docker/Dockerfile.notifier
    env_file: .env
    depends_on:
      - redis
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  redis_data:
```

### pyproject.toml

```toml
[project]
name = "flux-backend"
version = "1.0.0"
requires-python = ">=3.12"

dependencies = [
    "fastapi>=0.111.0",
    "uvicorn[standard]>=0.30.0",
    "pydantic>=2.0.0",
    "pydantic-settings>=2.0.0",
    "langgraph>=0.2.0",
    "langgraph-checkpoint-postgres>=0.1.0",
    "litellm>=1.40.0",
    "langsmith>=0.1.0",
    "sentry-sdk[fastapi]>=2.0.0",
    "supabase>=2.0.0",
    "asyncpg>=0.29.0",
    "apscheduler>=3.10.0",
    "sqlalchemy>=2.0.0",          # Required by APScheduler's PostgreSQL job store
    "twilio>=9.0.0",
    "pywebpush>=2.0.0",
    "slowapi>=0.1.9",
    "pendulum>=3.0.0",
    "python-dateutil>=2.9.0",
    "structlog>=24.0.0",
    "httpx>=0.27.0",
    "redis>=5.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "pytest-asyncio>=0.23.0",
    "httpx>=0.27.0",         # For TestClient
    "pytest-cov>=5.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
```

### Production Deployment (Railway / Render)

Deploy two services from the same repository:

**Service 1 — API:**
- Build command: `uv sync --frozen --no-dev`
- Start command: `uvicorn app.main:app --host 0.0.0.0 --port $PORT --workers 4`
- Environment: all variables from §4

**Service 2 — Notifier:**
- Build command: `uv sync --frozen --no-dev`
- Start command: `python -m notifier.main`
- Environment: all variables from §4
- `restart: always` — critical for notification reliability

**Redis:** Provision Redis as a managed add-on (Railway Redis or Render Redis). Set `REDIS_URL` accordingly.

**Database:** Supabase is external. No database service in Railway/Render — point `DATABASE_URL` and `SUPABASE_*` vars at the Supabase project.

---

## 18. Testing Strategy

### Unit Tests

Test each agent node in isolation by mocking the LiteLLM `llm_call` function and asserting that Pydantic models validate correctly against sample LLM outputs.

```python
# tests/unit/test_orchestrator.py
import pytest
from unittest.mock import AsyncMock, patch
from app.agents.orchestrator import orchestrator_node
from app.agents.state import AgentState

@pytest.mark.asyncio
async def test_orchestrator_classifies_goal():
    mock_response = '{"intent": "GOAL", "payload": {"goal": "I want to lose weight"}, "clarification_question": null, "task_id": null, "goal_id": null}'

    with patch("app.services.llm.llm_call", new=AsyncMock(return_value=mock_response)):
        state = AgentState(
            user_id="test-user",
            conversation_history=[{"role": "user", "content": "I want to lose weight"}],
            intent=None, user_profile={}, goal_draft=None, proposed_tasks=None,
            classifier_output=None, scheduler_output=None, pattern_output=None,
            approval_status=None, error=None, token_usage={}
        )
        result = await orchestrator_node(state)
        assert result["intent"] == "GOAL"
```

### Integration Tests

Use FastAPI's `TestClient` with a test Supabase project (separate from production). Test full API flows end-to-end with real DB writes. Use `pytest-asyncio` for async test support.

### Agent Retry Tests

Test that `validated_llm_call` correctly retries on malformed JSON and raises after `max_retries` exhausted.

### Notification Idempotency Tests

Test that the atomic CAS update pattern (`UPDATE ... WHERE ... IS NULL RETURNING id`) correctly prevents double-fire when two workers attempt to claim the same task simultaneously.

---

## 19. Phase 2 Roadmap

| Feature | Notes |
|---|---|
| React Native app | Reuse FastAPI backend entirely; replace PWA with RN |
| Google Calendar integration | Bi-directional sync via Google Calendar API; Scheduler reads external events as soft blocks |
| GPS geofencing | React Native location APIs + background geofencing; replace Demo Mode with real triggers |
| Celery + Redis workers | Replace APScheduler with Celery for horizontal scaling of the Notifier |
| Streak notifications | Push notifications for streak milestones: "You're on a 7-day streak! 🔥" |
| Apple Calendar / Outlook | After Google Calendar integration |
| Collaborative goals | Shared goals between users (accountability partner system) |
| Voice input | OpenAI Whisper API for voice-to-text in chat |
| iOS / Android widgets | Today's tasks on home screen widget |
| Embeddings-based dedup | `text-embedding-3-small` for semantic task deduplication before creating new tasks |

---

## 20. Open Decisions & Engineering Defaults

All defaults below were chosen based on the Architecture Review and product constraints. Review before each sprint.

| Decision | Default | Notes |
|---|---|---|
| Reminder lead time | 10 minutes | Configurable per user in `notification_preferences` |
| Escalation window | 2 minutes per channel | Configurable per user |
| Missed streak threshold | 3 consecutive misses, same slot | Hardcoded for MVP; move to config in Phase 2 |
| Max active goals | 3 | Prevents overload; configurable via `MAX_ACTIVE_GOALS` env var |
| Goal sprint length | 6 weeks | Fixed; per behavioral science grounding |
| RRULE expansion | Materialized (one row per occurrence) | Simpler for Notifier polling; watch DB growth |
| Max pipeline goals | No hard limit | Monitor table growth; add limit if needed |
| WhatsApp opt-in timing | During onboarding | Can be deferred to first escalation event; Meta approval required |
| LangGraph checkpoint store | `langgraph-checkpoint-postgres` on Supabase direct connection | Swap to Redis checkpointer at scale |
| Analytics refresh | Trigger-based on task status update | Materialized view concurrent refresh |
| Conversation summarization | At 20 messages or 8,000 tokens | Tune based on observed session lengths |
| Context window management | Sliding window with GPT-4o-mini summarization | Adjust split ratio if summary quality is low |
| Pattern Observer cold start | Chronotype-based defaults for first 2 weeks | Confidence < 0.5 until then |
| Token budget enforcement | Soft warn at 500k, degrade model at 1M | Adjust based on observed usage |
| APScheduler job store | SQLAlchemy PostgreSQL (Supabase direct) | Prevents job loss on restart |
| Notifier deployment | Separate Docker container, same repo | `restart: always` is non-negotiable |
| LiteLLM fallback | OpenRouter cross-model fallback (GPT-4o ↔ Claude Sonnet 4) | Verify fallback model pricing is acceptable via OpenRouter dashboard |

---

*This document is the single source of truth for the Flux backend. All agent prompts, data models, API contracts, infrastructure choices, and engineering defaults are subject to revision during sprint planning. Last updated: February 2026.*